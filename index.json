[{"uri":"https://ngtrkhanh.github.io/fcj-workshop-template-main/5-workshop/5.2-auth-setup/","title":"Configuring Google Cloud &amp; Amazon Cognito","tags":[],"description":"","content":" ‚öôÔ∏è Objective: Set up a Google Cloud Platform project to obtain OAuth credentials and configure Amazon Cognito User Pool as the centralized identity management system.\n1. Configuring Google Cloud Platform (GCP) To allow users to sign in with Gmail, we first need to create a project on Google Cloud and request OAuth 2.0 credentials.\nStep 1: Create an OAuth Client ID Go to Credentials ‚Üí Create Credentials ‚Üí OAuth client ID.\nApplication type: Web application Authorized redirect URIs: This is the address where Google will return the token after successful authentication. (This value will be obtained from the Amazon Cognito Domain in the next section.) Screenshot:\nFigure 5.2.2: Creating OAuth Client ID and Client Secret.\nNote: Make sure to save the Client ID and Client Secret for later use in the Cognito configuration.\n2. Configuring Amazon Cognito User Pool After obtaining credentials from Google, proceed to AWS Console to configure a User Pool.\nStep 1: Create User Pool \u0026amp; Identity Provider In the Amazon Cognito interface, create a new User Pool. Under Sign-in experience, select Federated identity providers and choose Google.\nFill in the Client ID and Client Secret obtained from Google Cloud.\nScreenshot:\nFigure 5.2.3: Entering Google authentication details into Cognito.\nStep 2: Configure App Client \u0026amp; Domain Under App integration:\nDomain: Create a Cognito Domain. This domain will be used as the Authorized redirect URI back in Google Cloud. App Client Settings: Allowed callback URLs: Your application‚Äôs frontend URL. OAuth 2.0 Grant Types: Select Authorization code grant. OpenID Connect scopes: Select email, openid, profile. Screenshot:\nFigure 5.2.5: Configuring Redirect URL and OAuth Scopes.\n3. Testing the Configuration (Hosted UI) To verify your configuration, open the Cognito-hosted Hosted UI.\nIf the \u0026ldquo;Continue with Google\u0026rdquo; button appears and functions correctly, the setup is successful.\nScreenshot:\nFigure 5.2.6: Login interface with Google successfully integrated.\n"},{"uri":"https://ngtrkhanh.github.io/fcj-workshop-template-main/5-workshop/5.5-frontend/5.5.1-api/","title":"Create API Gateway &amp; Authentication","tags":[],"description":"","content":" üõ°Ô∏è Goal: Create an HTTP API to consolidate multiple Lambda functions into a single endpoint and secure it using a Cognito Authorizer.\nStep 1: Create HTTP API Go to AWS Console \u0026gt; API Gateway. Choose HTTP API (low cost, high performance) \u0026gt; Click Build. API name: AuroraAPI. Click Next and leave the Integrations blank (we will add them later). Stage: Keep the default $default (Auto-deploy). Click Create. Illustration: Step 2: Connect Lambda (Integrations) We need to declare which Lambda functions this API will point to (created in section 5.4).\nGo to Integrations \u0026gt; Manage integrations \u0026gt; Create. Choose Lambda function. Select the function auroratimeEvent (or other functions you created). Repeat for other functions (auroratimeTodo, etc.). Illustration: Step 3: Create Routes Go to Routes \u0026gt; Create. Define API paths: POST /events -\u0026gt; Select integration auroratimeEvent GET /events -\u0026gt; Select integration auroratimeEvent POST /todos -\u0026gt; Select integration auroratimeTodo ‚Ä¶ (Note: Action logic can be handled inside the Lambda code or further divided by more detailed routes.) Illustration: Step 4: Configure CORS (Important) To allow the Frontend (Amplify) to call the API, enable CORS:\nGo to CORS. Access-Control-Allow-Origin: *. Access-Control-Allow-Methods: GET, POST, PUT, DELETE, OPTIONS. Access-Control-Allow-Headers: Content-Type, Authorization. Click Save. Illustration: Step 5: Configure Authentication (JWT Authorizer) This step secures the API. Only requests with a Cognito token are allowed.\nGo to Authorization \u0026gt; Manage authorizers tab \u0026gt; Create. Authorizer type: JWT. Name: CognitoAuth. Identity source: $request.header.Authorization. Issuer URL: https://cognito-idp.ap-southeast-1.amazonaws.com/ap-southeast-1_TryyHPjm0 (replace with your User Pool ID). Audience: 5dct7sk93a0unassp7komfpidq Click Create. Attach Authorizer: Go back to the Attach authorizers to routes tab, select routes (/events, /todos, ‚Ä¶) and assign CognitoAuth to them. Illustration: "},{"uri":"https://ngtrkhanh.github.io/fcj-workshop-template-main/5-workshop/5.4-backend-logic/5.4.4-resend/","title":"Resend &amp; Route 53 Configuration","tags":[],"description":"","content":" üìß Goal: Before writing the email-sending logic, we need to verify our domain (Domain Verification) to ensure outgoing emails do not land in Spam. We will connect Resend with AWS Route 53.\nStep 1: Add Domain to Resend Go to the Resend Dashboard. Click Add Domain. Enter your domain: auroratime.click. Select Region. Click Add. Resend will provide you with three types of DNS records (MX, SPF, DKIM). Illustration: Step 2: Configure DNS in AWS Route 53 We need to copy the DNS records from Resend and add them to Route 53.\nGo to AWS Console \u0026gt; Route 53 \u0026gt; Hosted zones. Select your domain. Click Create record. Create the MX record (Mail Exchange): Record name: (Leave empty or use the value provided by Resend) Record type: MX Value: Copy from Resend Create the TXT records (SPF \u0026amp; DKIM): Do the same for each TXT record required by Resend. Note: If the Record name ends with your domain, in Route 53 you only need to enter the prefix (e.g., bounces), because Route 53 will automatically append the domain. Illustration: Step 3: Verify and Get API Key Return to Resend and click Verify DNS Records. Wait around 1‚Äì5 minutes until the status turns Verified (Green). Go to API Keys \u0026gt; Create API Key. Name your key and select Sending access. Copy and store this API Key securely Image:\nImage: Illustration: üí° Tip: This DNS configuration improves your domain\u0026rsquo;s reputation, ensuring that system emails from Aurora land in the Inbox instead of Spam.\n"},{"uri":"https://ngtrkhanh.github.io/fcj-workshop-template-main/1-worklog/","title":"Worklog","tags":[],"description":"","content":"Typically, and as a standard, a worklog is carried out over about 3 months (throughout the internship period) with weekly contents as follows:\nWeek 1: Getting familiar with AWS and basic AWS services\nWeek 2: Doing task A\u0026hellip;\nWeek 3: Doing task B\u0026hellip;\nWeek 4: Doing task C\u0026hellip;\nWeek 5: Doing task D\u0026hellip;\nWeek 6: Doing task E\u0026hellip;\nWeek 7: Doing task G\u0026hellip;\nWeek 8: Doing task H\u0026hellip;\nWeek 9: Doing task I\u0026hellip;\nWeek 10: Doing task L\u0026hellip;\nWeek 11: Doing task M\u0026hellip;\nWeek 12: Doing task N\u0026hellip;\n"},{"uri":"https://ngtrkhanh.github.io/fcj-workshop-template-main/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"Launch of the Cloud Innovation Center at the University of Pittsburgh Every year, cloud technologies and artificial intelligence (AI) continue to reshape how universities, researchers, and public sector organizations solve their most complex challenges. In a milestone collaboration that is set to transform both health sciences research and athletic performance, the University of Pittsburgh (Pitt) and Amazon Web Services (AWS) have announced the launch of the Health Sciences and Sports Analytics Cloud Innovation Center (CIC) ‚Äî operated by AWS.\nThis is the first CIC on the East Coast of the United States and the first center dedicated specifically to health sciences and sports analytics, marking a significant advancement in how cloud technology and AI can be applied to these rapidly evolving fields.\nPioneering innovation in medicine and sports The new CIC will leverage AWS‚Äôs full suite of machine learning and AI tools, including Amazon SageMaker and Amazon Bedrock, to develop cutting-edge solutions for real-world challenges.\n‚ÄúAt AWS, we believe innovation happens when technology is paired with curious minds ready to solve big problems,‚Äù\n‚Äî Valerie Singer, Global Director for Education at AWS.\nShe added that combining Pitt‚Äôs deep expertise in health sciences with AWS‚Äôs cloud capabilities creates a powerful ‚Äúflywheel effect‚Äù that accelerates continuous innovation. Together, AWS and Pitt aim to improve medical outcomes and elevate student-athlete performance. This partnership represents exactly the type of collaboration capable of delivering breakthrough results.\nA hub for innovation in a transforming city The establishment of the new Cloud Innovation Center builds on Pittsburgh‚Äôs long-standing legacy of reinvention ‚Äî from a historic industrial hub to a modern center for technology, medicine, and data science.\nThrough this initiative:\nStudents gain hands-on experience with cloud and AI technologies, preparing them for leadership roles in the digital economy. Researchers gain access to scalable AWS infrastructure to accelerate groundbreaking discoveries in the health sciences. Athletic programs begin leveraging real-time data and analytics to enhance performance, strategy, and training outcomes. Open-source solutions developed within the CIC will benefit communities far beyond the campus. Student-led innovation Like the successful CICs at Arizona State University, University of British Columbia, and California Polytechnic State University, Pitt‚Äôs CIC will follow a student-driven innovation model.\nThrough rapid prototyping and iterative experimentation, students across previous CICs have already developed more than 150 open-source solutions addressing public sector challenges ‚Äî while building advanced technical skills and contributing tangible value to their communities.\nAt Pitt, enthusiasm is already high, with the first cohort of interns beginning their work in mid-April 2025.\n‚ÄúWorking with AWS and backend technologies has always fascinated me, and the CIC‚Äôs focus on solving real-world problems with these tools is exactly what drew me in,‚Äù\n‚Äî Mohammed Misran, CIC Intern.\n‚ÄúI‚Äôm excited to dive deeper and contribute to impactful projects. My goal isn‚Äôt just to develop technical skills ‚Äî it‚Äôs to maximize innovation and solve complex problems that traditional methods struggle to address‚Ä¶ for Pitt!‚Äù\nLooking ahead With its official launch today, the Pitt‚ÄìAWS CIC opens a promising new chapter in innovation for both health sciences and sports analytics.\nThe collaboration between AWS and the University of Pittsburgh is poised to accelerate the development of AI-driven solutions while preparing the next generation of technology leaders.\nAuthor\nAWS Public Sector Blog Team ‚Äî covering innovations in education, government, healthcare, and public sector transformation powered by AWS technologies.\n"},{"uri":"https://ngtrkhanh.github.io/fcj-workshop-template-main/4-eventparticipated/4.1-event1/","title":"AWS Cloud Day Vietnam - AI Edition 2025","tags":[],"description":"","content":"AWS Cloud Day Vietnam - AI Edition 2025 - Date: September 18, 2025 - Location: 2 Hai Trieu Street, Ben Nghe Ward, District 1, Ho Chi Minh City\nEvent Overview A pivotal gathering for Vietnam\u0026rsquo;s tech and business communities, focused on accelerating digital transformation through the convergence of Cloud Computing and Artificial Intelligence.\nKey Objectives:\nDemocratize Generative AI: Move GenAI from concept to practical, context-aware enterprise applications. Align Business \u0026amp; IT: Bridge the gap between business goals and IT, especially in Financial Services. Accelerate Modernization: Provide industry-specific roadmaps for migration and cloud-native development. Strengthen Security: Promote a \u0026ldquo;security by design\u0026rdquo; mindset across the application lifecycle. Key Takeaways \u0026amp; Learnings Data is the Differentiator: A comprehensive data strategy is a prerequisite for successful Generative AI. Modernization is a Continuous Journey: The goal is not just to migrate but to \u0026ldquo;Migrate to Operate\u0026rdquo; and innovate continuously. Business-Led Technology: Technology initiatives must be driven by clear business outcomes. Security is Everyone\u0026rsquo;s Responsibility: Security must be integrated from the first line of code. Application to Work Audit Data Readiness: Assess our current data strategy to ensure it can support future GenAI initiatives. Pilot GenAI in DevOps: Experiment with AI-driven code generation and automated testing to improve development velocity. Benchmark Modernization Efforts: Analyze case studies from Honda Vietnam (SAP migration) and Masterise Group (VMware migration) to refine our own modernization roadmaps. Implement \u0026ldquo;Security at Scale\u0026rdquo;: Integrate security tools and best practices throughout the entire development lifecycle. Event Photos "},{"uri":"https://ngtrkhanh.github.io/fcj-workshop-template-main/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"AI: Combine keyword \u0026amp; semantic search for text and images with Amazon Bedrock and Amazon OpenSearch Service Authors: Renan Bertolazzi, Birender Pal, Sarath Krishnan (24/04/2025) ‚Äî Amazon Bedrock, Amazon OpenSearch Service, Amazon Titan, Intermediate (200)\nShoppers want fast, accurate, and intuitive search. Great search improves conversion, AOV, and retention; 78% of users will buy again if experiences are personalized. With large catalogs, hybrid search is a strategic advantage.\nKeyword search: exact match on names/attributes. Semantic search: embeddings to capture meaning, resilient to phrasing, supports multimodal (text + image). Hybrid search: blends keyword + semantic scores; often improves quality 8‚Äì12% vs keyword and ~15% vs natural language (per OpenSearch). Amazon OpenSearch Service is the recommended vector database for Amazon Bedrock, delivering millisecond-latency vector search at scale. Amazon Titan Multimodal Embeddings G1 (via Bedrock) creates a shared embedding space for text and images, enabling cross-modal queries.\nGoal: Build a multimodal hybrid search solution that lets users submit text and/or images to retrieve relevant results from a sample retail image dataset.\nArchitecture overview Two main flows:\nIngestion: generate multimodal embeddings (text, image, metadata) with Bedrock (Titan) and store in OpenSearch. Query: OpenSearch search pipeline combines keyword + neural search; normalization processor standardizes and blends scores. Components:\nAmazon Bedrock (Titan Multimodal Embeddings G1) Amazon OpenSearch Service (vector + hybrid pipeline) Amazon S3 (source data) Amazon SageMaker Notebook (ingestion, testing) (Optional) SNS/SQS/Step Functions for orchestration Data ingestion Read text, images, metadata from S3; Base64-encode images. Send data to Bedrock (Titan) to get embeddings. Store embeddings + metadata into OpenSearch (bulk). Example index (shortened):\nsettings: index.knn: true number_of_shards: 2 mappings: properties: amazon_titan_multimodal_embeddings: type: knn_vector dimension: 1024 method: name: hnsw engine: lucene Query workflow Client sends text, Base64 image, or both. Pipeline runs: Keyword search (multi_match). Neural search (Titan embeddings for text/image). Normalization (e.g., min_max) + combination (arithmetic_mean) to blend scores. Return ranked results. Example search pipeline:\n{ \u0026#34;phase_results_processors\u0026#34;: [ { \u0026#34;normalization-processor\u0026#34;: { \u0026#34;normalization\u0026#34;: { \u0026#34;technique\u0026#34;: \u0026#34;min_max\u0026#34; }, \u0026#34;combination\u0026#34;: { \u0026#34;technique\u0026#34;: \u0026#34;arithmetic_mean\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;weights\u0026#34;: [OPENSEARCH_KEYWORD_WEIGHT, 1 - OPENSEARCH_KEYWORD_WEIGHT] } } } } ] } Hybrid query example:\n{ \u0026#34;query\u0026#34;: { \u0026#34;hybrid\u0026#34;: { \u0026#34;queries\u0026#34;: [ { \u0026#34;multi_match\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;\u0026lt;query_text\u0026gt;\u0026#34; } }, { \u0026#34;neural\u0026#34;: { \u0026#34;vector_embedding\u0026#34;: { \u0026#34;query_text\u0026#34;: \u0026#34;\u0026lt;query_text\u0026gt;\u0026#34;, \u0026#34;query_image\u0026#34;: \u0026#34;\u0026lt;base64_image\u0026gt;\u0026#34;, \u0026#34;model_id\u0026#34;: \u0026#34;\u0026lt;titan_connector_model_id\u0026gt;\u0026#34;, \u0026#34;k\u0026#34;: 5 } } } ] } } } Prerequisites AWS account. Bedrock with Amazon Titan Multimodal Embeddings G1 enabled. OpenSearch Service domain. SageMaker notebook. Familiarity with IAM, EC2, OpenSearch, SageMaker, Python. Sample code on GitHub. Create the Amazon Bedrock connector in OpenSearch Service IAM role allowing bedrock:InvokeModel for Titan Multimodal Embeddings G1; trust opensearchservice.amazonaws.com. Register model group, create connector, deploy model in OpenSearch (deploy=true). Build the search pipeline \u0026amp; enable hybrid search Use normalization (min_max) + combination (arithmetic_mean) to merge keyword/semantic scores. Tune OPENSEARCH_KEYWORD_WEIGHT as needed. Ingest sample data Select fields for embeddings; Base64 images; generate Titan embeddings; bulk into the index. Sample dataset: retail products (images + metadata). Create query functions for testing Keyword search: multi_match with query_text. Semantic search: neural with query_text, query_image, model_id. Hybrid search: combine both via the pipeline. Test observations Keyword: exact on names/attributes, misses context. Semantic: understands context, may overweight image vs. color. Hybrid: blended scoring surfaces the most relevant (right color + style). Cleanup Delete index, model deployment, model, model group, Bedrock connector; if you created a dedicated domain for the lab, delete it or remove test resources to avoid costs.\nConclusion Multimodal hybrid search on OpenSearch Service combines keyword + semantic strengths using Titan Multimodal Embeddings G1 (Bedrock). It supports text + image queries for more accurate, relevant results. Try the notebook, tune normalization/weights. For custom models on SageMaker, see ‚ÄúHybrid Search with Amazon OpenSearch Service‚Äù; for semantic-only, see Bedrock + OpenSearch guides for text and image search.\nAbout the authors Renan Bertolazzi ‚Äì Enterprise Solutions Architect at AWS, helps customers innovate and simplify. Birender Pal ‚Äì Senior Solutions Architect at AWS, focused on cloud-native, ML, GenAI. Sarath Krishnan ‚Äì Senior Solutions Architect at AWS, experienced in high availability and cost optimization; focuses on DevOps, ML, MLOps, generative AI. "},{"uri":"https://ngtrkhanh.github.io/fcj-workshop-template-main/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"AWS Big Data Blog Cross-account data collaboration with Amazon DataZone and AWS analytics tools Authors: Arun Pradeep Selvaraj, Piyush Mattoo, Mani Yamaraja ‚Äî March 5, 2025\nTopics: Amazon DataZone, Amazon Redshift, AWS Glue, Technical How-to, Thought Leadership\nData sharing is critical to innovation, growth, and collaboration. Gartner notes that organizations promoting data sharing outperform peers on business value metrics. A simple access-and-share mechanism is essential, yet cross-account permissions and finding the right data across accounts are common challenges when publishing and consuming data products on AWS.\nAmazon DataZone is a fully managed data management service to catalog, discover, share, and govern data across AWS.\nSolution overview This solution enables cross-account collaboration using Amazon DataZone domain association while preserving security and governance. We publish data assets via the Amazon DataZone business data catalog so other accounts can discover them, then query those assets from another AWS account using tools such as Amazon Athena and the Amazon Redshift query editor.\nProducer account: hosts data assets and the Amazon DataZone domain. Consumer account: needs access to producer data. Domain association uses AWS Resource Access Manager (AWS RAM). If both accounts are in the same AWS Organizations organization, association is automatic; otherwise AWS RAM sends an invitation for the consumer to accept or decline. Personas:\nData administrators: own producer/consumer; create the domain, configure/accept domain associations. Data publishers: in producer; create publish projects/environments, produce/publish assets, approve subscription requests. Data subscribers: in consumer; create subscribe projects/environments, discover/subscribe to assets, query data for insights. Prerequisites Two AWS accounts: producer and consumer. An Amazon Redshift provisioned cluster or Redshift Serverless workgroup in each account, provisioned by a data admin. A secret in AWS Secrets Manager storing master user credentials for each Redshift cluster/workgroup. Data admins create secrets; publishers/subscribers obtain secret ARNs when creating environments/profiles. Amazon DataZone uses Amazon Redshift datashares for cross-account sharing; both producer and consumer clusters must be encrypted and use supported RA3 types (ra3.16xlarge, ra3.4xlarge, ra3.xlplus) or Redshift Serverless. See datashare considerations for encryption. High-level steps:\nCreate Amazon DataZone domain in producer. Send domain association request from producer to consumer. Accept domain association in consumer. Add data users to the domain. Create publish projects for AWS Glue and Amazon Redshift (producer). Create Glue and Redshift environments to publish assets (producer). Create and run data sources for Glue/Redshift to publish assets to the business catalog. Create subscribe projects for Glue/Redshift (consumer). Create environment profiles/environments for Glue/Redshift in subscribe projects. Subscribe to Glue/Redshift tables and consume via Athena and Redshift query editor. Create Amazon DataZone domain (producer) Log in to the producer console as data admin. Create domain Demo_cross_account_domain (Quick setup enables default blueprints and environment profiles for data lake and data warehouse). Request domain association (producer ‚Üí consumer) In the producer Amazon DataZone console, open the domain, go to Associated Accounts, enter consumer account ID(s), choose Request association. Use policy AWS RAM DataZonePortalReadWrite so consumer users can call Amazon DataZone APIs and use the data portal. Accept domain association (consumer) Log in to consumer (same Region), open Amazon DataZone ‚Üí View requests ‚Üí select domain ‚Üí Review request ‚Üí Accept association. Domain shows associated. Enable environment blueprints: select domain ‚Üí Blueprints ‚Üí enable DefaultDataLake (provide Glue Manage Access role) and DefaultDataWarehouse. Add data users to the domain As data admin in producer: User management ‚Üí Add ‚Üí Add IAM users. Add publisher IAM users from the current account; add subscriber IAM users from the associated consumer account. Create publish projects (producer) Data publisher in producer creates projects in the data portal: Glue_Publish_Project (AWS Glue assets) Redshift_Publish_Project (Amazon Redshift assets) Create publish environments (producer) Glue In demo_cross_account_domain, open Glue_Publish_Project, create environment Glue_Publish_Environment with default DataLakeProfile. Leave producer_glue_db_name, consumer_glue_db_name, Workgroup_name blank. Open Athena (Analytics tools), ensure Glue_Publish_Environment and database glue_publish_environment_pub_db. Run CTAS to create sample table mkt_sls_table (sales/marketing sample). Verify the table exists. Redshift In Redshift_Publish_Project, create environment Redshift_Publish_Environment with default data warehouse profile. Provide Redshift cluster/workgroup, database name, and secret ARN. Secrets must include tags: Cluster: DataZone.rs.cluster: \u0026lt;cluster:db\u0026gt; Serverless: DataZone.rs.workgroup: \u0026lt;workgroup:db\u0026gt; AmazonDataZoneProject: \u0026lt;project_id\u0026gt; AmazonDataZoneDomain: \u0026lt;domain_id\u0026gt; In Redshift query editor, create table rs_sls_tbl via CTAS (sample sales data). Verify the table exists. Publish assets to the business data catalog (producer) Glue data source In Glue_Publish_Project ‚Üí Glue_Publish_Environment, create data source glue-publish-datasource, type AWS Glue, select environment, database glue_publish_environment_pub_db, table selection *. Run on demand; after run, mkt_sls_table appears. Review metadata ‚Üí Accept All ‚Üí Publish Asset. Redshift data source In Redshift_Publish_Project ‚Üí Redshift_Publish_Environment, create data source rs-publish-datasource, type Amazon Redshift. Provide cluster/workgroup and secret; select database dev, schema datazone_env_redshift_publish_environment. Run on demand; after run, rs_sls_tbl appears. Review metadata ‚Üí Accept All ‚Üí Publish Asset. Create subscribe projects (consumer) In consumer data portal, create: Glue_Subscribe_Project (for AWS Glue assets) Redshift_Subscribe_Project (for Amazon Redshift assets) Create environment profiles and environments (consumer) Glue Create environment profile glue_subscribe-env-profile: Blueprint Default Data Lake; account = consumer; Authorized projects = All; Publishing = Publish from any database. Create environment glue_subscribe_environment from the profile (optional: producer/consumer Glue DB names, workgroup). Wait until complete. Redshift In Redshift_Subscribe_Project, create profile redshift_subscribe-env-profile: Blueprint Default Data Warehouse; Parameter set = Enter my own; choose Cluster or Serverless; provide Redshift secret ARN; Authorized projects = All; Publishing = Publish any schema. Create environment redshift_subscribe_environment from the profile. Subscribe to AWS Glue and Amazon Redshift tables Glue subscription As data subscriber (consumer), open Glue_Subscribe_Project, select Market Sales Table (mkt_sls_table) ‚Üí Subscribe with justification. Publisher approves; subscriber confirms in Subscribed Assets. Open Amazon Athena (environment glue_subscribe_environment, DB glue_subscribe_environment_sub_db), preview mkt_sls_table and verify data. Redshift subscription In Redshift_Subscribe_Project, search Sales Table (rs_sls_tbl) ‚Üí Subscribe with justification. Publisher approves; subscriber confirms in Subscribed Assets. Open Redshift Query Editor V2 via project link, create a connection with temporary IAM credentials, select DB for the environment, and run:\nSELECT * FROM \u0026quot;dev\u0026quot;.\u0026quot;datazone_env_redshift_subscribe_environment\u0026quot;.\u0026quot;rs_sls_tbl\u0026quot;; Cleanup Delete Amazon DataZone projects created for the walkthrough. Delete the Amazon DataZone domain if it was created for the lab. Delete Redshift clusters/workgroups and related Secrets Manager secrets in both producer and consumer accounts. Summary Cross-account data sharing is complex due to permissions and security. This solution shows how to publish and consume data across AWS accounts using Amazon DataZone with AWS Glue and Amazon Redshift, maintaining consistent governance and secure access. You can monitor access/activity with AWS Lake Formation and CloudTrail (90-day default logs). Extend the pattern to more accounts as needed.\nAbout the authors Arun Pradeep Selvaraj ‚Äì Senior Solutions Engineer at AWS; focuses on digital transformation and cloud modernization. Piyush Mattoo ‚Äì Senior Solutions Architect (Financial Services) at AWS; 10+ years building distributed, scalable systems; MS CS (UMass); enjoys camping/hiking. Mani Yamaraja ‚Äì Senior Customer Solutions Manager (Financial Services) at AWS; 10+ years guiding customer cloud journeys; customer-obsessed, designs tech aligned to business goals. "},{"uri":"https://ngtrkhanh.github.io/fcj-workshop-template-main/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Nguyen Trong Khanh\nPhone Number: 0372863084\nEmail: nguyentrongkhanh18@gmail.com\nUniversity: FPT University\nMajor: Information Technology\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 12/08/2025 to 12/11/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://ngtrkhanh.github.io/fcj-workshop-template-main/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS infrastructure and how to use the AWS Management Console. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of the internship unit rules and regulations 08/09/2025 08/09/2025 https://policies.fcjuni.com/ 3 - Draw AWS architecture using draw.io 09/09/2025 09/09/2025 https://youtu.be/l8isyDe-GwY?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i 4 - Watch workshop tutorials 10/09/2025 10/09/2025 https://www.youtube.com/watch?v=mXRqgMr_97U\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=3\u0026pp=iAQB 5 - Learn basic theories about AWS infrastructure 11/09/2025 11/09/2025 6 - Practice: Set up Account and get familiar with AWS console 12/09/2025 12/09/2025 https://000001.awsstudygroup.com/vi/ Week 1 Achievements: Understood what AWS is. Successfully created and configured an AWS Free Tier account. Became familiar with the AWS Management Console and learned how to find, access, and use services via the web interface. "},{"uri":"https://ngtrkhanh.github.io/fcj-workshop-template-main/5-workshop/5.4-backend-logic/5.4.1-event/","title":"Event CRUD (Event Handler)","tags":[],"description":"","content":" üìÖ Function: This function handles all Create, Read, Update, and Delete (CRUD) operations for the user\u0026rsquo;s schedules/events.\nStep 1: Create Lambda Function Function Name: auroraTimeEvent Runtime: Node.js 18.x Description: API for handling CRUD operations for the Events table. Image: Step 2: Configure IAM Role (Full Access to Events) We need to grant full read/write permissions on the events table.\nJSON Policy (CloudWatch Logs permissions):\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:logs:ap-southeast-1:080563425614:*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:logs:ap-southeast-1:080563425614:log-group:/aws/lambda/auroraTimeEvent:*\u0026#34; ] } ] } ** JSON Policy (CRUD permissions for DynamoDB events table) { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;AllowCRUDOnEventsTable\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;dynamodb:PutItem\u0026#34;, \u0026#34;dynamodb:GetItem\u0026#34;, \u0026#34;dynamodb:UpdateItem\u0026#34;, \u0026#34;dynamodb:DeleteItem\u0026#34;, \u0026#34;dynamodb:Query\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:dynamodb:ap-southeast-1:080563425614:table/events\u0026#34; } ] } Step 3: Processing code (Node.js) Back to the Lambda Function interface, we will write Node.js code to process CRUD operations.\nH√¨nh ·∫£nh: After completing, click Deploy to save.\n"},{"uri":"https://ngtrkhanh.github.io/fcj-workshop-template-main/5-workshop/5.1-architecture/","title":"System Architecture &amp; Authentication Flow","tags":[],"description":"","content":" üìã Overview: This section describes the high-level architecture of the Aurora system and the authentication flow using Google OAuth 2.0 integrated with Amazon Cognito.\n1. High-Level System Architecture The Aurora system is built entirely on a Serverless architecture on AWS, optimizing both scalability and operational cost.\nIt integrates with Google Cloud Platform to provide a seamless Single Sign-On (SSO) experience via Google authentication.\nMain Components: Frontend (Client):\nA Web App (SPA) where users interact with the system, view schedules, and create tasks.\nAuthentication Layer:\nGoogle Cloud Project: Provides OAuth 2.0 Client ID/Secret to authenticate Gmail users. Amazon Cognito: Acts as the federated Identity Provider (IdP), managing the User Pool and issuing temporary AWS credentials to the Frontend. Backend Logic (Compute):\nAWS Lambda: Hosts business logic functions (Create Event, Update Tasks, Trigger Email Notifications). Database:\nAmazon DynamoDB: Stores Events and Daily Worklogs.\nUses UserId as the Partition Key to isolate each user\u0026rsquo;s data securely. Notification Service:\nEmail Sending Logic: Triggered by Lambda when a new event is created or when the scheduled time arrives.\nSends HTML-formatted emails using SES or a custom Email API. 2. Authentication Flow This flow ensures that only authenticated users can access their personal data.\nAurora uses Cognito Federated Identities combined with Google OAuth 2.0.\nStep-by-step Process: User Login:\nThe user clicks ‚ÄúSign in with Google‚Äù on the Frontend.\nGoogle OAuth:\nThe user is redirected to Google‚Äôs login page.\nAfter a successful login, Google returns an Id Token (JWT).\nToken Exchange:\nThe Frontend sends the Id Token to Amazon Cognito.\nVerification \u0026amp; Session Handling:\nCognito validates the Token with Google. If the token is valid:\nCognito creates/updates the corresponding user profile in the User Pool. Cognito returns temporary AWS credentials (Access Key, Secret Key, Session Token) to the Frontend. Authorized Requests:\nThe Frontend uses these credentials to access API resources (via API Gateway or directly invoking Lambda/DynamoDB through AWS SDK) with permissions defined in IAM Roles.\n3. Data Flow: Creating an Event \u0026amp; Sending Email Notifications When a user creates a new event (e.g., ‚ÄúTeam Meeting at 9:00 AM‚Äù), the data flow proceeds as follows:\nFrontend ‚Üí API Request:\nThe frontend sends a POST request containing the event details to the API endpoint.\nAWS Lambda Trigger:\nThe Lambda function is invoked and performs:\nInput validation Writing event data to DynamoDB (Table: AuroraEvents) Email Notification Trigger:\nAfter writing to DynamoDB, Lambda triggers the email-sending logic. The system generates an HTML email body. The Email Service (SES/Gmail API/Resend) sends the message to the user‚Äôs inbox. üí° Highlight: Integrating Google Login eliminates the need for users to manage an additional password, while benefiting from Google‚Äôs built-in two-factor authentication for enhanced security.\n"},{"uri":"https://ngtrkhanh.github.io/fcj-workshop-template-main/4-eventparticipated/4.2-event2/","title":"Discover Agentic AI ‚Äì Amazon QuickSuite Workshop","tags":[],"description":"","content":"Discover Agentic AI ‚Äì Amazon QuickSuite Workshop - Date: November 7, 2025 - Location: AWS Vietnam Office, Bitexco Financial Tower, HCMC\nEvent Overview An exclusive workshop focused on the shift from passive Generative AI to autonomous Agentic AI. The event featured the first live demonstration of Amazon QuickSuite in Vietnam and introduced the AWS LIFT Program to lower financial barriers for adoption.\nKey Objectives:\nDefine Agentic AI: Clarify the concept of autonomous AI agents that can reason and execute tasks. Introduce Amazon QuickSuite: Showcase the unified data visualization (QuickSight) and generative AI (Quick Suite Q) platform. Enable Hands-on Learning: Provide a practical environment to build AI concepts with expert guidance. Facilitate Adoption: Offer an $80,000 USD credit through the AWS LIFT Program to accelerate R\u0026amp;D. Key Takeaways \u0026amp; Learnings Focus on Autonomy: The design goal of Agentic AI is to build systems that act on a user\u0026rsquo;s behalf, not just provide information. Ecosystem Approach is Crucial: Effective agents require a connected network of tools, like the one provided by QuickSuite, to link data sources with action logic. Early Adoption Creates Advantage: Gaining proficiency with tools like QuickSuite before they become mainstream offers a significant competitive edge. Funding Accelerates Innovation: Financial incentives like the LIFT program enable companies to experiment and innovate more quickly. Application to Work Explore QuickSuite for Analytics: Investigate integrating QuickSight and Quick Suite Q to create \u0026ldquo;Analyst Agents\u0026rdquo; that can automate data reporting and analysis. Secure R\u0026amp;D Funding: Apply for the AWS LIFT Program to secure credits for upcoming AI-related research and development projects. Identify Automation Use Cases: Audit internal operations to find repetitive, multi-step tasks suitable for autonomous execution by an AI agent. Engage with Implementation Partners: Collaborate with partners like Cloud Kinetics for complex architectural design and implementation, reducing in-house development risks. Event Photos Add your event photos here\n"},{"uri":"https://ngtrkhanh.github.io/fcj-workshop-template-main/2-proposal/","title":"Proposal","tags":[],"description":"","content":"Aurora Time Executive Summary This proposal presents the implementation plan for Aurora Time, a time-management application built on AWS.\nThe goal is to provide a simple, intuitive, and cost-efficient scheduling tool for individual users.\nAurora Time leverages Serverless and Managed Services on AWS to achieve scalability, high reliability, and cost optimization ‚Äî delivering rapid return on investment (ROI) by eliminating infrastructure maintenance overhead.\nProblem Statement Current Problems Users struggle to manage their personal schedules because information is scattered across many tools (notes, phones, various apps).\nExisting solutions are often overly complex, business-oriented, and not suited for individual use.\nAurora Time provides a centralized, minimalistic, and intuitive solution that helps users easily manage habits and personal events.\nProposed Solution Aurora Time uses Amazon S3 and CloudFront to store and distribute the web application, and AWS Amplify for quick deployment.\nAmazon API Gateway and AWS Lambda handle backend CRUD operations, while DynamoDB provides fast and stable data storage.\nAmazon Cognito handles user authentication, and EventBridge \u0026amp; SES send automated reminders.\nKey Features:\nIntuitive scheduling interface Custom reminders Extremely low cost Benefits and Return on Investment (ROI) Aurora Time helps users save time, reduce scattered scheduling, and increase productivity.\nInfrastructure cost: $16 ‚Äì $50/month (~$192 ‚Äì $600/year) ROI: Under 6 months Advantages: No servers needed, ultra-low cost, highly scalable Solution Architecture Aurora Time applies an AWS Serverless architecture that allows flexible scaling from a single user to millions.\nRequests are received by Amazon API Gateway, processed by AWS Lambda, and stored in DynamoDB.\nEventBridge schedules and triggers reminders, AWS Amplify hosts the frontend, and Cognito provides security.\nAWS Services Used AWS Service Function AWS Lambda Handles CRUD business logic and reminders. Amazon API Gateway Provides secure RESTful APIs. Amazon DynamoDB Stores scheduling, event, and user data. Amazon S3 \u0026amp; CloudFront Stores and distributes the frontend content. Amazon EventBridge Schedules and triggers automated events. Amazon SES Sends reminder emails to users. AWS Amplify Hosts and manages the frontend. Amazon Cognito Secure user authentication and management. Technical Implementation Plan January ‚Äì Research \u0026amp; Architecture Design:\nDynamoDB modeling, Serverless architecture (API Gateway, Lambda, EventBridge). January ‚Äì POC \u0026amp; Cost Estimation:\nAWS Pricing Calculator, testing Cognito + DynamoDB. February ‚Äì System Optimization:\nTune Lambda (timeout, memory), optimize DynamoDB RCU/WCU. February‚ÄìMarch ‚Äì Development \u0026amp; CI/CD:\nBuild Lambda functions, set up CodePipeline + CodeBuild, develop React UI, Beta testing. Estimated Infrastructure Cost Service Description Monthly Cost (USD) AWS Amplify Static web hosting 0.35 S3 Static files \u0026amp; backup 0.05 CloudFront CDN (20GB) 1.70 API Gateway 30,000 requests 0.11 Lambda 1M requests (free tier) 0.00 DynamoDB 1GB data (free tier) 0.11 Cognito \u0026lt;1000 users 0.00 SES 500 emails/month 0.05 EventBridge 100k events 0.10 CloudWatch Logs 1GB logs 0.10 CI/CD Pipeline 20 builds 0.00 üëâ Total Estimated Cost: ~ $16 ‚Äì $50/month (~$192 ‚Äì $600/year)\nRisks \u0026amp; Mitigation Risk Impact Probability Mitigation Network outage Medium Medium Use cache \u0026amp; CDN (CloudFront). DynamoDB failure High Medium Conduct POC \u0026amp; load testing. Cost overrun Medium Low Set up AWS Budgets alerts. EventBridge/Lambda failure High Low Monitor via CloudWatch \u0026amp; retry logic. Expected Outcomes Simple, intuitive scheduling with automated reminders. Stable, low-cost, scalable serverless system. Full deployment during internship with automated CI/CD. "},{"uri":"https://ngtrkhanh.github.io/fcj-workshop-template-main/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material MON - Networking Essentials with Amazon VPC: review the concepts of VPC, Subnet, Route Table, Internet Gateway, NAT Gateway, Security Group, and NACL (high‚Äëlevel overview). 15/09/2025 15/09/2025 https://000003.awsstudygroup.com/vi/000003.awsstudygroup‚Äã TUE - Practice creating a custom VPC: create a VPC, 1 public subnet, 1 private subnet, attach an Internet Gateway, and configure the Route Table for the public subnet.- Document the CIDR blocks, AZs, and routes used for later EC2 labs. 16/09/2025 16/09/2025 https://000003.awsstudygroup.com/000003. https://www.youtube.com/watch?v=sllYqAECBoM\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=56awsstudygroup‚Äã WED - Compute Essentials with Amazon EC2: study the concepts of instance types, AMI, EBS volumes, Security Groups, key pairs, and basic pricing.000004.awsstudygroup‚Äã- Prepare a plan: choose AMI, instance type, subnet, and Security Group for the web server lab. 17/09/2025 17/09/2025 https://www.youtube.com/watch?v=yAR6QRT3N1k\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=74 THU - Attend an AWS event 18/09/2025 18/09/2025 FRI - Attach and configure an additional EBS volume for the EC2 instance (create, attach, format, and mount the volume).- Write a short internal note titled ‚ÄúVPC + EC2 Lab ‚Äì Week 2‚Äù describing the architecture, main steps, and security considerations (CIDR, Security Groups, key pairs). 19/09/2025 19/09/2025 https://www.youtube.com/watch?v=7NjNTnXon5s\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=44youtube‚Äã Week 2 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"},{"uri":"https://ngtrkhanh.github.io/fcj-workshop-template-main/5-workshop/5.4-backend-logic/5.4.2-todo/","title":"CRUD Todo (Tasks)","tags":[],"description":"","content":" üìÖ Function: This function handles all Create, Read, Update, and Delete (CRUD) operations for the user\u0026rsquo;s task list.\nStep 1: Create Lambda Function Function name: auroratimeTodo Runtime: Node.js 18.x Description: API that handles CRUD operations for the Todo table. Image: Step 2: Configure IAM Role (Full Access to Todo) We need to grant full CloudWatch and DynamoDB access to the todo table.\nJSON Policy (CloudWatch Logs):\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:logs:ap-southeast-1:080563425614:*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogTodo\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:logs:ap-southeast-1:080563425614:log-group:/aws/lambda/auroraTimeTodo:*\u0026#34; ] } ] } ```json ** JSON Policy (CRUD permissions for DynamoDB todo table) { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;AllowCRUDOnTodoTable\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;dynamodb:PutItem\u0026#34;, \u0026#34;dynamodb:GetItem\u0026#34;, \u0026#34;dynamodb:UpdateItem\u0026#34;, \u0026#34;dynamodb:DeleteItem\u0026#34;, \u0026#34;dynamodb:Query\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:dynamodb:ap-southeast-1:080563425614:table/todo\u0026#34; } ] } Step 3: Implement the Logic (Node.js) Return to the Lambda Function interface and begin writing your Node.js code to handle the CRUD operations.\nH√¨nh ·∫£nh: After completing, click Deploy to save your changes.\n"},{"uri":"https://ngtrkhanh.github.io/fcj-workshop-template-main/5-workshop/5.3-database/","title":"Database Design (Events &amp; Tasks)","tags":[],"description":"","content":" üóÑÔ∏è Objective: Design a NoSQL database using Amazon DynamoDB to store Events and Todo tasks, optimized for fast retrieval and low operational cost.\n1. Why Choose Amazon DynamoDB? With Aurora‚Äôs Serverless architecture, Amazon DynamoDB is the ideal choice because:\nServerless: No server management required, automatically scales based on workload. High performance: Low latency (single-digit milliseconds), ideal for real-time UI operations. Flexible (Schemaless): Easy to add new fields to Event/Todo items without complex migrations like SQL databases. 2. Schema Design (Data Modeling) The system follows a Per-User Isolation model.\nEvery item is associated with a userId (extracted from Cognito/Google tokens) to ensure data security.\nWe will create three main tables:\nTable 1: AuroraEvents (Stores calendar events) This table stores user events, displayed on the calendar and used for notification scanning.\nPartition Key (PK): userId (String) ‚Äî User identifier Sort Key (SK): eventId (String) ‚Äî UUID of the event Table 2: AuroraTasks (Stores daily tasks) This table stores the user‚Äôs Daily Worklog / Todo list.\nPartition Key (PK): userId (String) Sort Key (SK): todoId (String) ‚Äî UUID of the task Table 3: users (Stores user information) This table contains detailed information about each user.\nPartition Key (PK): userId (String) 3. Steps to Create Tables on AWS Console Below are the steps to create tables using the AWS DynamoDB Console.\nStep 1: Create the Events Table Navigate to DynamoDB ‚Üí Tables ‚Üí Create table.\nTable name: events Partition key: userId (String) Sort key: eventId (String) Screenshot:\nStep 2: Create the Tasks Table Repeat the process to create the Tasks table.\nTable name: todo Partition key: userId (String) Sort key: todoId (String) Screenshot:\nStep 3: Create the Users Table Finally, create the Users table.\nTable name: users Partition key: userId (String) Screenshot:\n"},{"uri":"https://ngtrkhanh.github.io/fcj-workshop-template-main/4-eventparticipated/4.3-event3/","title":"AWS Cloud Mastery Series #3 - Security Pillar Deep Dive","tags":[],"description":"","content":"AWS Cloud Mastery Series #3 - Security Pillar Deep Dive - Date: December 1, 2025 - Location: AWS Vietnam Office, Bitexco Financial Tower, HCMC\nEvent Overview An in-depth workshop focused on the Security Pillar of the AWS Well-Architected Framework. The event provided knowledge and best practices for securing cloud workloads.\nKey Objectives:\nDeep Dive into the Security Pillar: Analyze the design principles and key areas of security on AWS. Identity and Access Management: Gain a deep understanding of AWS IAM, MFA, and best practices for access control. Data Protection: Explore techniques for encrypting data at-rest and in-transit. Automation and Monitoring: Learn how to use AWS Config, CloudTrail, and Security Hub to monitor and automate security controls. Key Takeaways \u0026amp; Learnings Security is a Shared Responsibility: Understand the shared responsibility model and the customer\u0026rsquo;s role in securing applications in the cloud. Defense in Depth: Apply multiple layers of security to protect resources comprehensively. Automation is Key: Automating security checks and remediation reduces human error and allows for faster responses to threats. Application to Work Re-evaluate IAM Policies: Review and strengthen existing IAM policies according to the principle of least privilege. Implement Security Monitoring: Set up AWS Security Hub for a centralized, comprehensive view of the security posture. Enhance Data Encryption: Ensure all sensitive data is encrypted using AWS KMS. "},{"uri":"https://ngtrkhanh.github.io/fcj-workshop-template-main/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"This section lists the blogs you have translated:\nBlog 1 ‚Äì Launch of the Cloud Innovation Center at the University of Pittsburgh How Pitt and AWS launched the first East Coast CIC focused on health sciences and sports analytics, leveraging SageMaker and Bedrock to deliver student-led innovation.\nBlog 2 ‚Äì AI: Combine keyword \u0026amp; semantic search for text and images with Amazon Bedrock and Amazon OpenSearch Service Build multimodal hybrid search with Amazon Titan Multimodal Embeddings G1 and OpenSearch Service to blend keyword + semantic relevance for retail image search.\nBlog 3 ‚Äì Cross-account data collaboration with Amazon DataZone and AWS analytics tools Use Amazon DataZone with AWS Glue, Redshift, Athena, and RAM to publish, subscribe, and govern data products across AWS accounts with consistent security and governance.\n"},{"uri":"https://ngtrkhanh.github.io/fcj-workshop-template-main/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Week 3 s·∫Ω t·∫≠p trung v√†o RDS + Auto Scaling EC2 + CloudWatch, v√† Friday (T6) ghi r√µ b·∫°n tham gia s·ª± ki·ªán AWS/FCJ.\nWeek 3 ‚Äì Worklog (15‚Äì19/09/2025 pattern ti·∫øp t·ª•c) Day Task Start Date Completion Date Reference Material MON - Database Essentials with Amazon RDS: learn core concepts (DB instance, engine types, storage, Multi‚ÄëAZ, backup, security group for DB).[1] - Review common use cases of RDS for web applications. 22/09/2025 22/09/2025 https://www.youtube.com/watch?v=TQFwQAre0H4\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=59 TUE - Hands-on: create an RDS database instance in the same VPC as the EC2 web server from Week 2.\n- Configure security so that only the EC2 instance (or app subnet) can connect to the RDS instance. 23/09/2025 23/09/2025 https://www.youtube.com/watch?v=SlP-KdSs3IM\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=225 WED - Scaling Applications with EC2 Auto Scaling: learn the concepts of Launch Template/Configuration, Auto Scaling Group, scaling policies, health checks.[3] - Design a simple scaling policy based on CPU utilization for the web tier. 24/09/2025 24/09/2025 https://www.youtube.com/watch?v=hFVYG8WqfU0\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=80 THU - Hands-on: create an Auto Scaling Group for web EC2 instances across at least two AZs (if available).\n- Test scale‚Äëout/scale‚Äëin behavior by adjusting thresholds or generating load. 25/09/2025 25/09/2025 https://000006.awsstudygroup.com[3] FRI - Attend an AWS event 26/09/2025 26/09/2025 Week 3 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"},{"uri":"https://ngtrkhanh.github.io/fcj-workshop-template-main/5-workshop/5.4-backend-logic/","title":"Backend-Logic","tags":[],"description":"","content":" üõ°Ô∏è Goal: Build a Serverless Backend following a 4-step workflow: Create Function ‚Üí Configure Role ‚Üí Grant Database Permissions ‚Üí Implement Logic (Connect to DB \u0026amp; Call Third-Party Email API).\nStep 1: Initialize the Lambda Function First, we create a new Lambda function that will contain the processing logic.\nGo to AWS Console \u0026gt; Lambda \u0026gt; Create function. Function name: (enter your function name). Runtime: Choose Node.js 18.x (or 20.x). Architecture: x86_64. Keep the remaining settings as default and click Create function. Illustration:\nFigure 5.4.1: Creating the Backend processing Lambda function.\nStep 2: Add Policies to the Lambda (Execution Role) When the function is created, AWS automatically generates a basic IAM Role.\nWe need to access this Role and add permissions for writing to the Database.\nIn the Lambda function page, switch to the Configuration tab. Select Permissions on the left panel. Click the Role name under Execution role to open it in the IAM Console. Illustration:\nFigure 5.4.2: Accessing the IAM Role for permission configuration.\nStep 3: Add Policies for Database Access Since we use a third-party Email service (called via a normal HTTP API), we do not need SES permissions.\nWe only need to grant Lambda permissions to work with DynamoDB.\nIn the Role‚Äôs Permissions tab, click Add permissions \u0026gt; Create inline policy. Switch to the JSON mode. Click Next, name the policy AuroraDB_Access_Policy, and then click Create policy. Review your Permissions list to ensure the Role now has access to DynamoDB. Step 4: Write the Lambda Function Code Return to the Lambda Function interface and begin writing your Node.js code. Once completed, click Deploy to save and apply the changes.\n"},{"uri":"https://ngtrkhanh.github.io/fcj-workshop-template-main/5-workshop/5.4-backend-logic/5.4.3-sendreminder/","title":"Send Reminder (Notification)","tags":[],"description":"","content":" üîî Function: This function is designed to run on a schedule (e.g., every 5 minutes using EventBridge Scheduler). It scans the database for events happening within the next 15 minutes and sends reminder emails via a third-party API.\nStep 1: Create Lambda Function This function requires a slightly longer execution time since it needs to scan the database and wait for responses from the email API, so we will increase the Timeout.\nFunction name: sendReminder Runtime: Node.js 18.x Illustration:\nFigure 5.4.3.1: Configuring the background job Lambda function.\nStep 2: Configure IAM Role (Full Access to Events) JSON Policy:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:logs:ap-southeast-1:080563425614:*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:logs:ap-southeast-1:080563425614:log-group:/aws/lambda/sendReminderLambda:*\u0026#34; ] } ] } Step 3: Implementation Code (Node.js) Return to the Lambda Function interface, where we will write Node.js code to scan for upcoming events and send reminder emails via a third-party API.\nImage: After completing the code, click Deploy to save it.\n"},{"uri":"https://ngtrkhanh.github.io/fcj-workshop-template-main/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":"This section lists the events you have participated in during your internship. For example:\n4.1. AWS Cloud Day Vietnam - AI Edition 2025 Date: September 18, 2025\nLocation: 2 Hai Trieu Street, Ben Nghe Ward, District 1, Ho Chi Minh City\nA pivotal gathering for Vietnam\u0026rsquo;s tech and business communities, focused on accelerating digital transformation through the convergence of Cloud Computing and Artificial Intelligence.\n4.2. Discover Agentic AI ‚Äì Amazon QuickSuite Workshop Date: November 7, 2025\nLocation: AWS Vietnam Office, Bitexco Financial Tower, HCMC\nAn exclusive workshop focused on the shift from passive Generative AI to autonomous Agentic AI. The event featured the first live demonstration of Amazon QuickSuite in Vietnam and introduced the AWS LIFT Program to lower financial barriers for adoption.\n4.3. AWS Cloud Mastery Series #3 - Security Pillar Deep Dive Add your event details here\n"},{"uri":"https://ngtrkhanh.github.io/fcj-workshop-template-main/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: D∆∞·ªõi ƒë√¢y l√† Week 4 (kh√¥ng c√≥ event), v·∫´n b√°m theo flow Cloud9 + S3 static website.\nWeek 4 ‚Äì Worklog Day Task Start Date Completion Date Reference Material MON - Set up Hybrid DNS with Route 53 Resolver: learn basic concepts of Route 53, Route 53 Resolver, hybrid DNS between on‚Äëprem and AWS, inbound/outbound endpoints, and Resolver rules (introduction).- Take notes about typical hybrid DNS use cases for enterprise environments. 06/10/2025 06/10/2025 https://www.youtube.com/watch?v=FGicpWOmMDI\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=46youtube‚Äã TUE - Follow the Hybrid DNS lab: create required VPCs, subnets and initial Route 53 configuration for the lab scenario.- Draw a small architecture diagram showing on‚Äëprem DNS, Route 53 Resolver inbound/outbound endpoints, and VPCs. 07/10/2025 07/10/2025 https://000010.awsstudygroup.com/vi/ WED - Configure Security Groups for the Hybrid DNS lab: keep only ICMP (ping) and RDP ports needed for testing, remove unused ports to follow least‚Äëprivilege and improve security.- Test connectivity (ping, RDP) to ensure Security Group rules work as expected. 08/10/2025 08/10/2025 https://www.youtube.com/watch?v=kE_krznNBFU\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=49youtube THU - Complete Hybrid DNS with Route 53 Resolver: configure inbound and outbound Resolver endpoints and create Resolver rules to forward specific domains between on‚Äëprem and AWS.- Test DNS resolution in both directions (from ‚Äúon‚Äëprem‚Äù to AWS and from AWS back to on‚Äëprem). 09/10/2025 09/10/2025 https://www.youtube.com/watch?v=L-2YfZceoAU\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=61000003.awsstudygroup FRI - Set up AWS Transit Gateway: learn concepts and pricing (attachments, data processing) and create a Transit Gateway in the lab account.- Attach existing VPCs to the Transit Gateway and update VPC route tables so traffic between VPCs flows through the TGW; test cross‚ÄëVPC connectivity. 10/10/2025 10/10/2025 https://www.youtube.com/watch?v=W1m_OFPDui0\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=68 https://www.youtube.com/watch?v=QSXgL2KodQI\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i Week 4 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"},{"uri":"https://ngtrkhanh.github.io/fcj-workshop-template-main/5-workshop/","title":"Workshop","tags":[],"description":"","content":" üí° Project Information: Aurora is an event-management and automated notification system built on a Serverless architecture.\nProject Aurora: Event Management \u0026amp; Automated Notification System Project Overview Project Aurora is a solution that enables users to manage schedules, create important events, and track daily tasks (Daily Worklog).\nA key highlight of the system is its ability to integrate automated email notifications, reminding users when an event is about to occur.\nThis project focuses on solving the problem of creating schedules and delivering reliable notifications without maintaining traditional servers.\nKey features and services include:\nAmazon Cognito \u0026amp; Google Cloud: Centralized user identity management with Google Sign-In (OAuth 2.0) for secure and simplified authentication. Event \u0026amp; Task Management: Built using Amazon DynamoDB to store event details and daily task statuses. Email Notification System: Integrated with Resend to deliver beautiful HTML-formatted emails to users. Logic Processing: Implemented with AWS Lambda to handle workflow execution whenever a new event is created. Detailed Sections Google Cloud \u0026amp; Amazon Cognito Configuration System Architecture \u0026amp; Authentication Flow Database Design (Events \u0026amp; Tasks) Building API \u0026amp; Email Logic (Lambda) User Interface (Frontend) Final Results \u0026amp; Future Improvements "},{"uri":"https://ngtrkhanh.github.io/fcj-workshop-template-main/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Master the concepts and practice Serverless architecture using AWS Lambda. Learn about user authentication with Amazon Cognito. Build a basic CI/CD pipeline with AWS CodePipeline. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Automation using Serverless with AWS Lambda 06/10/2025 06/10/2025 https://000022.awsstudygroup.com/vi/ 3 - Cross-domain authentication with Amazon Cognito 07/10/2025 07/10/2025 https://000141.awsstudygroup.com/vi/ 4 - CI/CD Pipeline with AWS CodePipeline 08/10/2025 08/10/2025 https://000017.awsstudygroup.com/vi/ 5 - Frontend Development for Serverless API 09/10/2025 09/10/2025 https://000079.awsstudygroup.com/vi/ 6 - Build Front end React 10/10/2025 10/10/2025 \u0026lt;\u0026gt; Week 5 Achievements: ‚òÅÔ∏è Automation (Lambda): Mastered the Serverless concept and how AWS Lambda operates. Successfully created and deployed a basic Lambda Function. Understood how to configure Triggers (event sources) for Lambda functions. Authentication (Cognito): Learned about the Amazon Cognito service and its role in user identity management. Successfully created and configured a basic Cognito User Pool for managing sign-ups/sign-ins. Development and Deployment (CI/CD): Understood the Continuous Integration/Continuous Delivery (CI/CD) process. Built a simple CI/CD Pipeline using AWS CodePipeline to automate code deployment. End-to-End Serverless Architecture: Understood and practiced building a basic Serverless Backend by integrating Lambda (logic), S3 (static/frontend storage), and DynamoDB (NoSQL database). Gained knowledge on developing a Frontend to communicate with the built Serverless API. "},{"uri":"https://ngtrkhanh.github.io/fcj-workshop-template-main/5-workshop/5.5-frontend/","title":"Frontend &amp; API Gateway","tags":[],"description":"","content":" üåê Architecture: We will build a standard secure system: Frontend (Amplify) -\u0026gt; API Gateway (with Cognito authentication) -\u0026gt; Lambda Backend.\nConnection Model To ensure security and centralized management, the Frontend is not allowed to call Lambda directly. Instead, we use Amazon API Gateway.\nWorkflow:\nDeploy: The ReactJS Frontend is hosted on AWS Amplify. Authenticate: Users log in via Google/Cognito and receive an IdToken. Request: The Frontend sends a request to API Gateway with the Authorization Header containing the Token. Authorize: API Gateway verifies the Token with Cognito. If valid -\u0026gt; forwards the request to Lambda. Execute: Lambda executes the logic and returns the result. "},{"uri":"https://ngtrkhanh.github.io/fcj-workshop-template-main/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Complete Frontend‚ÄìBackend Serverless Integration: Understand how to build the Frontend interface and implement API calls from the Frontend to API Gateway. Set Up System Monitoring: Practice creating and using Amazon CloudWatch to build dashboards that monitor system performance. Prepare for the Final Project: Finalize the project idea, assign tasks among team members, and begin building the core components of the project. Tasks for the Week: Day Task Start Date Completion Date Reference 2 - Serverless: Guide to building Frontend that calls API Gateway 13/10/2025 13/10/2025 https://000079.awsstudygroup.com/vi/ 3 - Create system monitoring dashboards with Amazon CloudWatch 14/10/2025 14/10/2025 https://000008.awsstudygroup.com/vi/ 4 - 15/10/2025 15/10/2025 5 - Team meeting to prepare and finalize final project ideas 16/10/2025 16/10/2025 6 - Practice: Start building the project 17/10/2025 17/10/2025 Week 6 Achievements: Completed Serverless Frontend Integration: Successfully studied and practiced building the Frontend (UI) capable of sending API requests to API Gateway (following the provided guide). Set Up System Monitoring: Practiced creating and configuring CloudWatch Dashboards to monitor basic system resources and performance indicators. Kick-off for the Final Project: Participated in a team meeting to discuss and finalize the idea for the final project. Started building the initial components of the project (e.g., folder structure, initial AWS service setup). Strengthened Serverless Knowledge: Gained a clearer understanding of Serverless architecture flow, especially the interaction between Frontend, API Gateway, and Backend services. Improved Team Collaboration Skills: Actively contributed to discussions and task planning within the team in preparation for the major project. "},{"uri":"https://ngtrkhanh.github.io/fcj-workshop-template-main/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"I participated in the \u0026ldquo;Cessation Support Platform\u0026rdquo; project, where I significantly strengthened and expanded my technical capabilities across multiple domains:\nAPI Development \u0026amp; Integration: Built and managed secure and scalable APIs using Amazon API Gateway, ensuring efficient communication between frontend and backend services.\nAuthentication \u0026amp; User Management: Implemented user authentication and authorization with AWS Cognito, enabling secure sign-up, sign-in, and session handling.\nFrontend Development: Contributed to building the web interface by integrating APIs, handling user interactions, and ensuring smooth communication between the client side and backend services.\nCloud Deployment: Leveraged AWS Amplify to deploy and manage the frontend application, improving deployment speed and simplifying environment configuration.\nDatabase Management: Designed and maintained a non-relational database using MongoDB, managing collections for user data, social interactions, and platform activity logs.\nAnalysis \u0026amp; Problem-Solving: Interpreted project requirements, tackled technical challenges such as API integration issues and data-handling complexities, and delivered practical solutions to maintain system stability.\nRegarding my professional conduct, I consistently aimed to complete my tasks with high quality, followed established team workflows, and maintained proactive communication with teammates to ensure the project moved in the right direction.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ‚òê ‚úÖ ‚òê 2 Ability to learn Ability to absorb new knowledge and learn quickly ‚òê ‚úÖ ‚òê 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ‚òê ‚úÖ ‚òê 4 Sense of responsibility Completing tasks on time and ensuring quality ‚úÖ ‚òê ‚òê 5 Discipline Adhering to schedules, rules, and work processes ‚òê ‚úÖ ‚òê 6 Progressive mindset Willingness to receive feedback and improve oneself ‚òê ‚úÖ ‚òê 7 Communication Presenting ideas and reporting work clearly ‚òê ‚úÖ ‚òê 8 Teamwork Working effectively with colleagues and participating in teams ‚úÖ ‚òê ‚òê 9 Professional conduct Respecting colleagues, partners, and the work environment ‚úÖ ‚òê ‚òê 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ‚òê ‚úÖ ‚òê 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ‚úÖ ‚òê ‚òê 12 Overall General evaluation of the entire internship period ‚úÖ ‚òê ‚òê Needs Improvement Strengthen personal discipline and maintain consistent adherence to organizational rules and regulations. Develop sharper problem-solving skills and the ability to approach challenges with a more analytical mindset. Enhance communication abilities in both daily interactions and professional settings, especially in managing and responding to situations effectively. "},{"uri":"https://ngtrkhanh.github.io/fcj-workshop-template-main/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Advanced Review: Complete the in-depth review of the 4 pillars of the AWS Well-Architected Framework to prepare for the midterm exam. Knowledge Systematization: Understand the roles and operational mechanisms of the key services (IAM, KMS, Multi-AZ, Lambda, CloudFront) within each pillar. Mindset \u0026amp; Preparation: Maintain a relaxed mindset and be ready for the midterm exam. Tasks to Complete This Week: Day Tasks Start Date End Date Resources 2 - Review Secure Architectures: IAM, KMS, Security Groups, NACLs, Secrets Manager 20/10/2025 20/10/2025 3 - Review Resilient Architectures: Multi-AZ, Auto Scaling, Route 53, DR Strategies 21/10/2025 21/10/2025 4 - Review High-Performing Architectures: Lambda, Caching, CloudFront, EC2 Auto Scaling, S3 (Storage Tiering). 22/10/2025 22/10/2025 5 - Continue reviewing High-Performing Architectures: Lambda, Caching, CloudFront, EC2 Auto Scaling, S3 (Storage Tiering). 23/10/2025 23/10/2025 6 - Rest and prepare mentally for the midterm exam 24/10/2025 24/10/2025 Week 7 Achievements: Completed Review Plan: Completed the in-depth review of all 4 pillars of the AWS Well-Architected Framework (Security, Reliability, Performance Efficiency, Cost Optimization) according to the schedule.\nKnowledge Systematization: Successfully understood and categorized the roles of the core services (IAM, KMS, Multi-AZ, Auto Scaling, Lambda, CloudFront, S3) within each architectural pillar.\nFoundation Strengthening: Reviewed fundamental AWS service groups (Compute, Storage, Networking, Database, \u0026hellip;) and practiced using AWS Console/CLI.\nExam Preparation: Summarized and systematized knowledge, and stayed mentally prepared for the midterm exam.\n"},{"uri":"https://ngtrkhanh.github.io/fcj-workshop-template-main/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":"Overall Evaluation 1. Working Environment\nThe work environment is exceptionally welcoming and open. FCJ team members consistently offer support whenever I face challenges, even beyond regular working hours. The workspace is well-organized and comfortable, which significantly enhances my ability to concentrate.\n2. Support from Mentor / Team Admin\nThe mentor delivers comprehensive guidance, provides clear explanations when I\u0026rsquo;m uncertain, and consistently encourages me to ask questions. The team mentor assists with various issues I encounter, ensuring smooth workflow. I particularly value the mentor\u0026rsquo;s approach of allowing me to experiment and independently resolve problems rather than simply providing solutions.\n3. Alignment Between Work and Academic Field\nThe assignments I received match the knowledge I acquired during my university studies, while simultaneously introducing me to entirely new domains I hadn\u0026rsquo;t explored before. As a result, I\u0026rsquo;ve been able to reinforce my foundational understanding while acquiring hands-on practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nThroughout the internship period, I\u0026rsquo;ve gained numerous new competencies including project management tool usage, collaborative teamwork abilities, and professional communication within a corporate setting. The mentor has also shared extensive real-world insights that have better guided my career direction.\n5. Culture \u0026amp; Team Spirit\nThe company culture is highly positive: mutual respect among colleagues, serious work ethic, and collective effort with mutual support.\n6. Policies / Benefits for Interns\nThe company provides flexible scheduling arrangements when necessary. Additionally, the opportunity to participate in internal training sessions represents a significant advantage.\nAdditional Questions What were you most satisfied with during your internship?\nThe most satisfying aspect was the final four weeks, when I had the opportunity to build a complete microservices system (Program Service and Chat Service) from scratch. Witnessing the complex business logic I designed‚Äîsuch as the Streak system and Slip/Relapse mechanisms‚Äîfunctioning accurately was an incredibly rewarding experience. It transformed theoretical knowledge into a tangible, functional product.\nWhat do you think the company should improve for future interns?\nI believe the internship program would be more effective with a structured training roadmap from the beginning, helping interns better understand the company\u0026rsquo;s actual work and their role within projects. Receiving step-by-step guidance, from work processes to implementing real-world tasks, would enable interns to integrate more quickly into the professional work environment.\nFurthermore, having clearer direction on essential DevOps skills, as well as an overview of the AWS services the company utilizes, would help interns build a solid foundation for long-term growth. I also hope the program could provide detailed documentation or competency frameworks to guide those interested in pursuing DevOps roles in the future.\nIf recommending to friends, would you encourage them to intern here? Why?\nAbsolutely. I would highly recommend it to others. This isn\u0026rsquo;t an internship where you\u0026rsquo;re limited to trivial tasks. You\u0026rsquo;re entrusted with a real, challenging project and given the trust to build it. The learning curve is steep, but the practical experience you gain in cloud architecture, backend development, and professional software engineering practices is invaluable. The dedicated support from mentors creates an ideal environment for growth.\nSuggestions \u0026amp; Expectations What suggestions do you have to improve the internship experience?\nI suggest expanding the office space to allow more interns the opportunity to interact directly with each other and experience working in an office environment more frequently.\nWould you like to continue with this program in the future?\nYes, I\u0026rsquo;m very interested in continuing with FCJ, whether through another internship program. I feel a strong connection to the project I\u0026rsquo;ve built and the infrastructure deployed on AWS, and I\u0026rsquo;m excited about the possibility of further developing it‚Äîpotentially implementing features, optimizing workflows, or the Video Call functionality mentioned in the original proposal‚Äîwhile continuing personal growth and learning new valuable skills.\nOther Comments (free sharing):\nI\u0026rsquo;m truly grateful for this opportunity. This internship has been a pivotal moment in my professional development. It not only strengthened my technical skills but also gave me the confidence to tackle complex real-world software engineering challenges. Thank you to the mentor and the entire team for their guidance and support.\n"},{"uri":"https://ngtrkhanh.github.io/fcj-workshop-template-main/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference only. Please do not copy it verbatim for your own report, including this warning.\nWeek 8 Goals: Start implementing the actual project using AWS serverless services. Build a basic backend workflow using DynamoDB, Lambda, and API Gateway. Register a domain and configure DNS via Route 53 for API exposure. Tasks for This Week: Day Tasks Start Date Completion Date Reference 2 - Analyze project requirements - Define backend workflow: API Gateway ‚Üí Lambda ‚Üí DynamoDB 27/10/2025 27/10/2025 3 - Create DynamoDB Table + Define Partition Key + Insert sample items 28/10/2025 28/10/2025 4 - Register domain via Route 53 + Purchase domain + Create Public Hosted Zone + Add A/CNAME records for routing setup 29/10/2025 29/10/2025 5 - Write Lambda functions for API + Implement CRUD operations with DynamoDB SDK + Create IAM Role for Lambda 30/10/2025 30/10/2025 6 - Create API Gateway \u0026amp; integrate with Lambda + Set up REST API + Add GET/POST methods + Deploy stage + Test endpoint 31/10/2025 31/10/2025 Week 8 Achievements: Successfully created the DynamoDB Table for the project:\nDefined the Partition Key Added sample data to test workflow Verified functionality via Query/Scan Completed Route 53 domain registration, including:\nCreating a Public Hosted Zone Adding DNS records (A, CNAME) Verifying DNS propagation Built and deployed an AWS Lambda function:\nImplemented logic for GET/POST requests Integrated DynamoDB SDK for read/write operations Configured appropriate IAM Role permissions Set up API Gateway:\nCreated REST API with resources and methods Integrated with Lambda using Lambda Proxy Deployed the dev stage Successfully tested API endpoint Completed full serverless workflow integration: Client ‚Üí API Gateway ‚Üí Lambda ‚Üí DynamoDB\nStrengthened understanding of real-world serverless architecture and how services tie together to form a working backend.\n‚Ä¶\n"},{"uri":"https://ngtrkhanh.github.io/fcj-workshop-template-main/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference only. Please do not copy it verbatim into your report, including this warning.\nWeek 9 Goals: Integrate Google Authentication into Amazon Cognito (Google as an Identity Provider). Configure Amazon SES to send emails (notifications, verification, OTP). Test full flow: user login ‚Üí Cognito authentication ‚Üí SES email delivery. Tasks for This Week: Day Tasks Start End Reference 2 - Prepare Cognito environment - Create new User Pool \u0026amp; App Client for Google Login 03/11/2025 03/11/2025 3 - Configure Google OAuth: + Create OAuth Client ID in GCP + Retrieve Client ID \u0026amp; Secret 04/11/2025 04/11/2025 4 - Integrate Google with Cognito: + Create Google Identity Provider + Attribute Mapping + Test login flow 05/11/2025 05/11/2025 5 - Set up Amazon SES: + Domain verification + Sender email verification + DKIM setup + Remove SES Sandbox limitation 06/11/2025 06/11/2025 6 - Build SES email Lambda + Integrate with API Gateway + Test sending email via REST API 07/11/2025 07/11/2025 Week 9 Achievements: Successfully integrated Google Login with Amazon Cognito:\nCreated User Pool \u0026amp; App Client Set up Google OAuth in GCP Added Google as an Identity Provider in Cognito Verified login \u0026amp; token exchange workflow Fully configured Amazon SES:\nDomain and email verification DKIM enabled for improved email deliverability SES successfully moved out of sandbox Able to send verified emails Built a Lambda function for email sending via SES:\nSupports OTP / verification / notification emails Integrated with API Gateway for external triggering Verified entire authentication + email workflow: Google Sign-in ‚Üí Cognito Authentication ‚Üí Lambda ‚Üí SES Email Delivery\nImproved understanding of OAuth provider integration \u0026amp; email service configuration within AWS.\n‚Ä¶\n"},{"uri":"https://ngtrkhanh.github.io/fcj-workshop-template-main/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Goals: Find an alternative email solution after SES sandbox removal request was rejected. Switch to Resend as the primary email delivery service. Configure API authentication (API Key / JWT) to protect the email endpoint. Rebuild the email workflow using Resend. Tasks for This Week: Day Tasks Start End Reference 2 - Received SES sandbox removal rejection - Evaluate project email requirements and possible alternatives 10/11/2025 10/11/2025 3 - Research Resend: + Generate API Key + Domain verification + Explore email API usage 11/11/2025 11/11/2025 https://resend.com/docs 4 - Integrate Resend with Lambda: + Use SDK/HTTP API + Create email templates + Test email sending 12/11/2025 12/11/2025 5 - Configure API Authentication: + API Key or + JWT via Cognito + Enable auth in API Gateway 13/11/2025 13/11/2025 6 - End-to-End Testing: + Client ‚Üí API Gateway ‚Üí Lambda ‚Üí Resend + Validate tokens + Debug logs in CloudWatch 14/11/2025 14/11/2025 CloudWatch Logs Week 10 Achievements: SES sandbox removal request was rejected, causing limitations:\nUnable to send emails to unverified addresses Not suitable for production usage Required switching to an external provider Successfully migrated to Resend:\nCreated API Key Verified domain for better deliverability Sent emails successfully using Resend API Integrated Lambda + Resend:\nSupports HTML templates Email types: OTP, verification, notification (Optional) Stored API Key securely in AWS Secrets Manager Implemented API Authentication:\nAPI Key or JWT Cognito authentication API Gateway now requires authentication to trigger Lambda Improved security of email API endpoint Completed full workflow testing: Client ‚Üí Authenticated API Gateway ‚Üí Lambda ‚Üí Resend ‚Üí Email Delivered\nStrengthened understanding of external email providers and API authentication methods.\n‚Ä¶\n"},{"uri":"https://ngtrkhanh.github.io/fcj-workshop-template-main/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The content below is for reference only. Please do not copy it verbatim into your report.\nWeek 11 Goals: Integrate frontend with backend using AWS Amplify. Use Cognito for authentication (login, JWT, API Key) in frontend. Integrate Lambda + Resend + API Gateway into frontend via Amplify API. Deploy full CI/CD automatically via Amplify Console. Tasks for This Week: Day Tasks Start End Reference 2 - Configure Amplify project + amplify init + Connect frontend (React/Vue/Angular) + Add Amplify Auth (Cognito User Pool) 17/11/2025 17/11/2025 AWS Amplify Docs 3 - Integrate Lambda + Resend API via Amplify API + Create REST/GraphQL endpoint + Configure authentication using Cognito JWT/API Key 18/11/2025 18/11/2025 Amplify Docs, Cognito Docs 4 - Test frontend ‚Üí Amplify API ‚Üí Lambda ‚Üí Resend + Verify Cognito login works + Send OTP/notification emails from frontend 19/11/2025 19/11/2025 Project Docs 5 - Deploy CI/CD via Amplify Console + Connect GitHub/GitLab/CodeCommit repo + Auto build \u0026amp; deploy frontend + backend Lambda 20/11/2025 20/11/2025 Amplify Console Docs 6 - Test end-to-end pipeline + Push code ‚Üí Amplify auto deploy + Check frontend UI, API authentication, and email sending via Resend 21/11/2025 21/11/2025 CloudWatch Logs Week 11 Achievements: Fully integrated frontend with backend via Amplify:\nCall Lambda + Resend API from frontend Cognito authentication (login, JWT, API Key) works OTP/verification/notification emails sent successfully Deployed fully automated CI/CD via Amplify Console:\nRepo as source (GitHub/GitLab/CodeCommit) Auto build \u0026amp; deploy frontend + backend Production updated automatically on push End-to-end workflow tested: Frontend ‚Üí Cognito Auth ‚Üí Amplify API ‚Üí Lambda ‚Üí Resend ‚Üí Email ‚Üí Frontend displays notification\nLearned how to integrate serverless full-stack with Amplify + Cognito + Lambda + Resend and CI/CD automation.\n‚Ä¶\n"},{"uri":"https://ngtrkhanh.github.io/fcj-workshop-template-main/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives: Review and re-check the entire project before submission. Complete the proposal document and materials for the workshop. Prepare the presentation slides for the final report. Tasks Completed This Week: Day Task Start Finish Source 2 - Reviewed the entire source code + Checked logic + Verified UI/UX + Validated backend‚Äìfrontend flow 24/11/2025 24/11/2025 ‚Äî 3 - Finalized the proposal + Wrote system description + Refined the overall architecture section 25/11/2025 25/11/2025 ‚Äî 4 - Prepared workshop content + Technical slides + Functionality demo 26/11/2025 26/11/2025 Workshop Notes 5 - Designed presentation slides + Optimized content + Standardized illustrations and graphics 27/11/2025 27/11/2025 ‚Äî Week 12 Achievements: The entire project was reviewed and checked for issues:\nAll features are running smoothly No major bugs found Demo runs flawlessly for workshop and presentation The proposal was fully completed:\nClear structure Comprehensive system description, architecture, and workflow Formatted according to course requirements Workshop materials were prepared:\nTechnical explanation slides Complete demo flow Presentation slides were finalized:\nClear, concise content Consistent visuals and formatting Ready for the final presentation "},{"uri":"https://ngtrkhanh.github.io/fcj-workshop-template-main/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://ngtrkhanh.github.io/fcj-workshop-template-main/tags/","title":"Tags","tags":[],"description":"","content":""}]